{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment PAMAP2 with mcfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment finds an optimal model for the PAMAP2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# mcfly\n",
    "from mcfly import tutorial_pamap2, modelgen, find_architecture, storage\n",
    "# Keras module is use for the deep learning\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "# We can set some backend options to avoid NaNs\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = '/media/sf_VBox_Shared/timeseries/PAMAP2_Dataset/slidingwindow512cleaned/'\n",
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "ext = '.npy'\n",
    "for i in range(9):\n",
    "    Xs.append(np.load(datapath+'X_train_'+str(i)+ext))\n",
    "    ys.append(np.load(datapath+'y_train_binary'+str(i)+ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create a model architecture. As we do not know what architecture is best for our data we will create a set of models to investigate which architecture is most suitable for our data and classification task. You will need to specificy how many models you want to create with argument 'number_of_models', the type of model which can been 'CNN' or 'DeepConvLSTM', and maximum number of layers per modeltype. See for a full overview of the optional arguments the function documentation of modelgen.generate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = ys[0].shape[1]\n",
    "np.random.seed(123)\n",
    "models = modelgen.generate_models(Xs[0].shape,\n",
    "                                  number_of_classes=num_classes,\n",
    "                                  number_of_models = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filters': array([27, 93, 67, 96, 57, 83, 42]),\n",
       " 'learning_rate': 0.013854217299751215,\n",
       " 'lstm_dims': array([35]),\n",
       " 'regularization_rate': 0.02086630923723395}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "Now that the model architectures have been generated it is time to compare the models by training them in a subset of the training data and evaluating the models in the validation subset. This will help us to choose the best candidate model. Performance results are stored in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define directory where the results, e.g. json file, will be stored\n",
    "resultpath = '/media/sf_VBox_Shared/timeseries/PAMAP2_Dataset/results/' \n",
    "if not os.path.exists(resultpath):\n",
    "        os.makedirs(resultpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(X_list, y_list, j):\n",
    "    X_train = np.concatenate(X_list[0:j]+X_list[j+1:])\n",
    "    X_test = X_list[j]\n",
    "    y_train = np.concatenate(y_list[0:j]+y_list[j+1:])\n",
    "    y_test = y_list[j]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def split_train_small_val(X_list, y_list, j, trainsize=500, valsize=500):\n",
    "    X = np.concatenate(X_list[0:j]+X_list[j+1:])\n",
    "    y = np.concatenate(y_list[0:j]+y_list[j+1:])\n",
    "    rand_ind = np.random.choice(X.shape[0], trainsize+valsize, replace=False)\n",
    "    X_train = X[rand_ind[:trainsize]]\n",
    "    y_train = y[rand_ind[:trainsize]]\n",
    "    X_val = X[rand_ind[trainsize:]]\n",
    "    y_val = y[rand_ind[trainsize:]]\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def get_fresh_copy(model, lr):\n",
    "    model_json = model.to_json()\n",
    "    model_copy = model_from_json(model_json)\n",
    "    model_copy.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=['accuracy'])\n",
    "    #for layer in model_copy.layers:\n",
    "    #    layer.build(layer.input_shape)\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [(get_fresh_copy(model, params['learning_rate']), params, model_type)  for model, params, model_type in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8\n",
      "Training model 0 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 156s - loss: 4.8019 - acc: 0.1540 - val_loss: 2.5160 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 148s - loss: 3.6550 - acc: 0.1300 - val_loss: 2.3931 - val_acc: 0.1200\n",
      "Epoch 3/10\n",
      "480/500 [===========================>..] - ETA: 5s - loss: 2.7390 - acc: 0.1854 Epoch 00002: early stopping\n",
      "500/500 [==============================] - 158s - loss: 2.7197 - acc: 0.1860 - val_loss: 2.4210 - val_acc: 0.1840\n",
      "Training model 1 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 119s - loss: 3.1419 - acc: 0.0940 - val_loss: 2.4291 - val_acc: 0.1200\n",
      "Epoch 2/10\n",
      "480/500 [===========================>..] - ETA: 3s - loss: 2.6169 - acc: 0.1063Epoch 00001: early stopping\n",
      "500/500 [==============================] - 116s - loss: 2.6125 - acc: 0.1100 - val_loss: 2.4442 - val_acc: 0.0760\n",
      "Training model 2 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 112s - loss: 3.2211 - acc: 0.1000 - val_loss: 2.5904 - val_acc: 0.0600\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 111s - loss: 3.1540 - acc: 0.1000 - val_loss: 2.5800 - val_acc: 0.1200\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 108s - loss: 2.9774 - acc: 0.1120 - val_loss: 2.4854 - val_acc: 0.1560\n",
      "Epoch 4/10\n",
      "480/500 [===========================>..] - ETA: 3s - loss: 2.7951 - acc: 0.1250Epoch 00003: early stopping\n",
      "500/500 [==============================] - 109s - loss: 2.7968 - acc: 0.1260 - val_loss: 2.5758 - val_acc: 0.0960\n",
      "Training model 3 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 20s - loss: 3.6243 - acc: 0.5180 - val_loss: 1.6063 - val_acc: 0.4420\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 20s - loss: 3.2784 - acc: 0.7420 - val_loss: 0.9591 - val_acc: 0.7560\n",
      "Epoch 3/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 2.6482 - acc: 0.8125Epoch 00002: early stopping\n",
      "500/500 [==============================] - 20s - loss: 2.6497 - acc: 0.8100 - val_loss: 0.9758 - val_acc: 0.7580\n",
      "Training model 4 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 16s - loss: 2.3181 - acc: 0.4620 - val_loss: 1.4404 - val_acc: 0.5680\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 17s - loss: 1.7403 - acc: 0.7460 - val_loss: 1.2840 - val_acc: 0.6900\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 16s - loss: 1.4919 - acc: 0.8480 - val_loss: 1.1746 - val_acc: 0.7160\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 16s - loss: 1.3544 - acc: 0.8900 - val_loss: 1.0703 - val_acc: 0.7660\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 17s - loss: 1.3183 - acc: 0.9180 - val_loss: 1.0598 - val_acc: 0.7660\n",
      "Epoch 6/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 1.2721 - acc: 0.9333Epoch 00005: early stopping\n",
      "500/500 [==============================] - 16s - loss: 1.2688 - acc: 0.9360 - val_loss: 1.0684 - val_acc: 0.7600\n",
      "Training model 5 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 30s - loss: 43.9846 - acc: 0.3540 - val_loss: 2.3024 - val_acc: 0.3640\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 30s - loss: 24.2146 - acc: 0.5780 - val_loss: 2.1080 - val_acc: 0.3420\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 30s - loss: 8.9518 - acc: 0.5900 - val_loss: 1.7470 - val_acc: 0.3740\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 30s - loss: 4.8301 - acc: 0.6460 - val_loss: 1.6131 - val_acc: 0.4740\n",
      "Epoch 5/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 3.4633 - acc: 0.6687Epoch 00004: early stopping\n",
      "500/500 [==============================] - 31s - loss: 3.4702 - acc: 0.6560 - val_loss: 1.7409 - val_acc: 0.4600\n",
      "Training model 6 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 133s - loss: 11.7960 - acc: 0.2260 - val_loss: 2.5639 - val_acc: 0.1200\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 176s - loss: 3.9058 - acc: 0.3260 - val_loss: 2.2099 - val_acc: 0.2100\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 187s - loss: 2.8524 - acc: 0.3640 - val_loss: 1.5692 - val_acc: 0.5100\n",
      "Epoch 4/10\n",
      "480/500 [===========================>..] - ETA: 4s - loss: 2.9466 - acc: 0.3354Epoch 00003: early stopping\n",
      "500/500 [==============================] - 152s - loss: 2.9498 - acc: 0.3340 - val_loss: 1.7827 - val_acc: 0.2800\n",
      "Training model 7 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 254s - loss: 11.6596 - acc: 0.0840 - val_loss: 2.4453 - val_acc: 0.1560\n",
      "Epoch 2/10\n",
      "480/500 [===========================>..] - ETA: 8s - loss: 3.0726 - acc: 0.1250 Epoch 00001: early stopping\n",
      "500/500 [==============================] - 251s - loss: 3.0519 - acc: 0.1260 - val_loss: 2.4572 - val_acc: 0.0760\n",
      "Training model 8 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 14s - loss: 81.2538 - acc: 0.5060 - val_loss: 2.0186 - val_acc: 0.4300\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 15s - loss: 10.5769 - acc: 0.6500 - val_loss: 1.5477 - val_acc: 0.4380\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 14s - loss: 4.1812 - acc: 0.6920 - val_loss: 1.2565 - val_acc: 0.6160\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 16s - loss: 3.5301 - acc: 0.6940 - val_loss: 0.9975 - val_acc: 0.7220\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 14s - loss: 3.5100 - acc: 0.6580 - val_loss: 0.9418 - val_acc: 0.7120\n",
      "Epoch 6/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 3.3187 - acc: 0.7125Epoch 00005: early stopping\n",
      "500/500 [==============================] - 13s - loss: 3.3166 - acc: 0.7080 - val_loss: 1.2075 - val_acc: 0.6100\n",
      "Training model 9 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 232s - loss: 16.9842 - acc: 0.1460 - val_loss: 2.2395 - val_acc: 0.3140\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 236s - loss: 16.0859 - acc: 0.3060 - val_loss: 2.0134 - val_acc: 0.5360\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 208s - loss: 15.3125 - acc: 0.4180 - val_loss: 1.8241 - val_acc: 0.6760\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 230s - loss: 14.5130 - acc: 0.5180 - val_loss: 1.6803 - val_acc: 0.6540\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 238s - loss: 13.8346 - acc: 0.5600 - val_loss: 1.5326 - val_acc: 0.6440\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 243s - loss: 13.1969 - acc: 0.5800 - val_loss: 1.4719 - val_acc: 0.6300\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 220s - loss: 12.6361 - acc: 0.6240 - val_loss: 1.3429 - val_acc: 0.6820\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 224s - loss: 12.0851 - acc: 0.6540 - val_loss: 1.2520 - val_acc: 0.6800\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 232s - loss: 11.5850 - acc: 0.6600 - val_loss: 1.2236 - val_acc: 0.6880\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 230s - loss: 11.1972 - acc: 0.6640 - val_loss: 1.1136 - val_acc: 0.6940\n",
      "Training model 10 DeepConvLSTM\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 431s - loss: 7.5087 - acc: 0.2200 - val_loss: 2.0973 - val_acc: 0.4480\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 755s - loss: 6.3928 - acc: 0.4420 - val_loss: 1.7179 - val_acc: 0.6520\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 517s - loss: 5.4129 - acc: 0.5700 - val_loss: 1.4997 - val_acc: 0.6760\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 298s - loss: 4.6062 - acc: 0.6820 - val_loss: 1.2359 - val_acc: 0.7160\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 277s - loss: 4.0675 - acc: 0.7220 - val_loss: 1.0536 - val_acc: 0.7720\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 288s - loss: 3.5249 - acc: 0.7740 - val_loss: 0.9424 - val_acc: 0.7760\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 281s - loss: 3.1019 - acc: 0.8280 - val_loss: 0.8674 - val_acc: 0.8160\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 345s - loss: 2.8752 - acc: 0.8080 - val_loss: 0.8095 - val_acc: 0.7740\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 301s - loss: 2.5976 - acc: 0.8400 - val_loss: 0.7368 - val_acc: 0.8100\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 269s - loss: 2.3980 - acc: 0.8500 - val_loss: 0.6748 - val_acc: 0.8100\n",
      "Training model 11 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 6s - loss: 2.8446 - acc: 0.6100 - val_loss: 1.2816 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 6s - loss: 3.1854 - acc: 0.7880 - val_loss: 0.9025 - val_acc: 0.7540\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 6s - loss: 2.7407 - acc: 0.8320 - val_loss: 0.7207 - val_acc: 0.8700\n",
      "Epoch 4/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 2.2707 - acc: 0.9042Epoch 00003: early stopping\n",
      "500/500 [==============================] - 6s - loss: 2.2785 - acc: 0.9000 - val_loss: 0.7767 - val_acc: 0.7860\n",
      "Training model 12 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 29s - loss: 545.6969 - acc: 0.2240 - val_loss: 2.4926 - val_acc: 0.1200\n",
      "Epoch 2/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 46.5763 - acc: 0.3708Epoch 00001: early stopping\n",
      "500/500 [==============================] - 29s - loss: 45.3780 - acc: 0.3700 - val_loss: 2.8309 - val_acc: 0.1720\n",
      "Training model 13 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 13s - loss: 297.3973 - acc: 0.2220 - val_loss: 2.1564 - val_acc: 0.2560\n",
      "Epoch 2/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 30.0776 - acc: 0.3104Epoch 00001: early stopping\n",
      "500/500 [==============================] - 13s - loss: 29.3005 - acc: 0.3100 - val_loss: 2.5554 - val_acc: 0.1160\n",
      "Training model 14 CNN\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 14s - loss: 2.1211 - acc: 0.4780 - val_loss: 2.0564 - val_acc: 0.4160\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 14s - loss: 1.6461 - acc: 0.6740 - val_loss: 1.0624 - val_acc: 0.6720\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 14s - loss: 1.4215 - acc: 0.7540 - val_loss: 0.9056 - val_acc: 0.7200\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 14s - loss: 1.1955 - acc: 0.8280 - val_loss: 0.7851 - val_acc: 0.7780\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 13s - loss: 1.0500 - acc: 0.8560 - val_loss: 0.7064 - val_acc: 0.7940\n",
      "Epoch 6/10\n",
      "480/500 [===========================>..] - ETA: 0s - loss: 1.0020 - acc: 0.8417Epoch 00005: early stopping\n",
      "500/500 [==============================] - 14s - loss: 0.9960 - acc: 0.8460 - val_loss: 0.7271 - val_acc: 0.8020\n",
      "9513.906688451767\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "np.random.seed(123)\n",
    "histories_list, val_accuracies_list, val_losses_list = [], [], []\n",
    "for j in range(8, len(Xs)):\n",
    "    print('fold '+str(j))\n",
    "    models = [(get_fresh_copy(model, params['learning_rate']), params, model_type)  for model, params, model_type in models]\n",
    "    X_train, y_train, X_val, y_val = split_train_small_val(Xs, ys, j, trainsize=500, valsize=500)\n",
    "    histories, val_accuracies, val_losses = find_architecture.train_models_on_samples(X_train, y_train,\n",
    "                                                                           X_val, y_val,\n",
    "                                                                           models,\n",
    "                                                                           nr_epochs=10,\n",
    "                                                                           subset_size=500,\n",
    "                                                                           verbose=True,\n",
    "                                                                           outputfile=resultpath+\\\n",
    "                                                                                  'experiment'+str(j)+'.json',\n",
    "                                                                           early_stopping=True)\n",
    "    histories_list.append(histories)\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    val_losses.append(val_losses)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read them all back in\n",
    "import json\n",
    "model_jsons = []\n",
    "for j in range(len(Xs)):\n",
    "    with open(resultpath+'experiment'+str(j)+'.json', 'r') as outfile:\n",
    "        model_jsons.append(json.load(outfile))\n",
    "model_jsons[0] = model_jsons[0][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['regularization_rate', 'val_acc', 'modeltype', 'lstm_dims', 'learning_rate', 'train_loss', 'train_acc', 'val_loss', 'filters'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_jsons[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "val_accuracies = np.array([[mod['val_acc'][-1] for mod in fold] for fold in model_jsons])\n",
    "[print(len(a)) for a in val_accuracies];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_acc = np.array([np.array([mod['val_acc'][-1] for mod in fold], dtype='float') for fold in model_jsons])\n",
    "train_acc = np.array([np.array([mod['train_acc'][-1] for mod in fold], dtype='float') for fold in model_jsons])\n",
    "train_loss = np.array([np.array([mod['train_loss'][-1] for mod in fold], dtype='float') for fold in model_jsons])\n",
    "val_loss = np.array([np.array([mod['val_loss'][-1] for mod in fold], dtype='float') for fold in model_jsons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17644445,  0.136     ,  0.14977778,  0.77866667,  0.77911111,\n",
       "        0.39733334,  0.20022222,  0.11422222,  0.60088889,  0.74888889,\n",
       "        0.75511112,  0.81466667,  0.18844445,  0.10666667,  0.76155556])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies_avg = val_acc.mean(axis=0)\n",
    "val_accuracies_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_acc = np.array([[history.history['acc'][-1] for history in histories] for histories in histories_list])\n",
    "# train_loss = np.array([[history.history['loss'][-1] for history in histories] for histories in histories_list])\n",
    "# val_acc = np.array([[history.history['val_acc'][-1] for history in histories] for histories in histories_list])\n",
    "# val_loss = np.array([[history.history['val_loss'][-1] for history in histories] for histories in histories_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of comparing model performance is by putting all the information in a pandas dataframe, which we can store in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'regularization_rate': 0.02086630923723395, '...</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>2.898308</td>\n",
       "      <td>0.176444</td>\n",
       "      <td>2.403033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'regularization_rate': 0.0022288077746968685,...</td>\n",
       "      <td>0.131556</td>\n",
       "      <td>2.698856</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>2.393217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'regularization_rate': 0.00017892532817003035...</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>2.613123</td>\n",
       "      <td>0.149778</td>\n",
       "      <td>2.409913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'regularization_rate': 0.0017941745604499644,...</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>2.186001</td>\n",
       "      <td>0.778667</td>\n",
       "      <td>0.844342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'regularization_rate': 0.001089490983523001, ...</td>\n",
       "      <td>0.927556</td>\n",
       "      <td>1.244737</td>\n",
       "      <td>0.779111</td>\n",
       "      <td>0.992753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'regularization_rate': 0.006174523760141131, ...</td>\n",
       "      <td>0.585111</td>\n",
       "      <td>8.875916</td>\n",
       "      <td>0.397333</td>\n",
       "      <td>2.180829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'regularization_rate': 0.07596584041274666, '...</td>\n",
       "      <td>0.229556</td>\n",
       "      <td>3.538028</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>2.270040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'regularization_rate': 0.0967007204303304, 'l...</td>\n",
       "      <td>0.122667</td>\n",
       "      <td>2.769499</td>\n",
       "      <td>0.114222</td>\n",
       "      <td>2.484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'regularization_rate': 0.016069244803607875, ...</td>\n",
       "      <td>0.674889</td>\n",
       "      <td>3.624188</td>\n",
       "      <td>0.600889</td>\n",
       "      <td>1.283478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'regularization_rate': 0.025948092548121634, ...</td>\n",
       "      <td>0.681778</td>\n",
       "      <td>11.305308</td>\n",
       "      <td>0.748889</td>\n",
       "      <td>1.073372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'regularization_rate': 0.007895753120779645, ...</td>\n",
       "      <td>0.734444</td>\n",
       "      <td>2.648503</td>\n",
       "      <td>0.755111</td>\n",
       "      <td>0.895082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'regularization_rate': 0.00029035321282643397...</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>2.102188</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.681575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'regularization_rate': 0.07769823179414435, '...</td>\n",
       "      <td>0.298889</td>\n",
       "      <td>28.988075</td>\n",
       "      <td>0.188444</td>\n",
       "      <td>3.264543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'regularization_rate': 0.006592800931749509, ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>20.744892</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>5.792905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'regularization_rate': 0.0005821148860525382,...</td>\n",
       "      <td>0.816222</td>\n",
       "      <td>1.153795</td>\n",
       "      <td>0.761556</td>\n",
       "      <td>0.841634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  train_acc  train_loss  \\\n",
       "0   {'regularization_rate': 0.02086630923723395, '...   0.264000    2.898308   \n",
       "1   {'regularization_rate': 0.0022288077746968685,...   0.131556    2.698856   \n",
       "2   {'regularization_rate': 0.00017892532817003035...   0.170000    2.613123   \n",
       "3   {'regularization_rate': 0.0017941745604499644,...   0.834000    2.186001   \n",
       "4   {'regularization_rate': 0.001089490983523001, ...   0.927556    1.244737   \n",
       "5   {'regularization_rate': 0.006174523760141131, ...   0.585111    8.875916   \n",
       "6   {'regularization_rate': 0.07596584041274666, '...   0.229556    3.538028   \n",
       "7   {'regularization_rate': 0.0967007204303304, 'l...   0.122667    2.769499   \n",
       "8   {'regularization_rate': 0.016069244803607875, ...   0.674889    3.624188   \n",
       "9   {'regularization_rate': 0.025948092548121634, ...   0.681778   11.305308   \n",
       "10  {'regularization_rate': 0.007895753120779645, ...   0.734444    2.648503   \n",
       "11  {'regularization_rate': 0.00029035321282643397...   0.872222    2.102188   \n",
       "12  {'regularization_rate': 0.07769823179414435, '...   0.298889   28.988075   \n",
       "13  {'regularization_rate': 0.006592800931749509, ...   0.300000   20.744892   \n",
       "14  {'regularization_rate': 0.0005821148860525382,...   0.816222    1.153795   \n",
       "\n",
       "     val_acc  val_loss  \n",
       "0   0.176444  2.403033  \n",
       "1   0.136000  2.393217  \n",
       "2   0.149778  2.409913  \n",
       "3   0.778667  0.844342  \n",
       "4   0.779111  0.992753  \n",
       "5   0.397333  2.180829  \n",
       "6   0.200222  2.270040  \n",
       "7   0.114222  2.484940  \n",
       "8   0.600889  1.283478  \n",
       "9   0.748889  1.073372  \n",
       "10  0.755111  0.895082  \n",
       "11  0.814667  0.681575  \n",
       "12  0.188444  3.264543  \n",
       "13  0.106667  5.792905  \n",
       "14  0.761556  0.841634  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelcomparisons = pd.DataFrame({'model':[str(params) for model, params, model_types in models],\n",
    "                       'train_acc': train_acc.mean(axis=0),\n",
    "                       'train_loss': train_loss.mean(axis=0),\n",
    "                       'val_acc': val_acc.mean(axis=0),\n",
    "                       'val_loss': val_loss.mean(axis=0)\n",
    "                       })\n",
    "modelcomparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to vizualize the performance of the various models using our vizualisation tool as explained in the mcfly repository README file: https://github.com/NLeSC/mcfly/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check which model is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type and parameters of the best model:\n",
      "CNN\n",
      "{'regularization_rate': 0.00029035321282643397, 'fc_hidden_nodes': 196, 'learning_rate': 0.006326663929636657, 'filters': array([35, 16, 64])}\n"
     ]
    }
   ],
   "source": [
    "best_model_index = np.argmax(val_accuracies_avg)\n",
    "best_model, best_params, best_model_types = models[best_model_index]\n",
    "print('Model type and parameters of the best model:')\n",
    "print(best_model_types)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/media/sf_VBox_Shared/timeseries/PAMAP2_Dataset/results/bestmodel_sample_architecture.json',\n",
       " '/media/sf_VBox_Shared/timeseries/PAMAP2_Dataset/results/bestmodel_sample_weights')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'bestmodel_sample'\n",
    "storage.savemodel(best_model,resultpath,modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model for real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the best model architecture out of our random pool of models we can continue by training the model on the full training sample. For the purpose of speeding up the example we only train the full model on the first 1000 values. You will need to replace this by 'datasize = X_train.shape[0]' in a real world example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14663 samples, validate on 2155 samples\n",
      "Epoch 1/2\n",
      "14663/14663 [==============================] - 289s - loss: 1.3279 - acc: 0.8916 - val_loss: 0.9985 - val_acc: 0.8418\n",
      "Epoch 2/2\n",
      "14663/14663 [==============================] - 300s - loss: 0.4954 - acc: 0.9351 - val_loss: 1.0915 - val_acc: 0.7968\n",
      "Train on 14528 samples, validate on 2290 samples\n",
      "Epoch 1/2\n",
      "14528/14528 [==============================] - 286s - loss: 1.3542 - acc: 0.8890 - val_loss: 1.2473 - val_acc: 0.5581\n",
      "Epoch 2/2\n",
      "14528/14528 [==============================] - 301s - loss: 0.5336 - acc: 0.9257 - val_loss: 1.2055 - val_acc: 0.5934\n",
      "Train on 15344 samples, validate on 1474 samples\n",
      "Epoch 1/2\n",
      "15344/15344 [==============================] - 343s - loss: 1.3298 - acc: 0.8914 - val_loss: 0.3195 - val_acc: 0.9166\n",
      "Epoch 2/2\n",
      "15344/15344 [==============================] - 355s - loss: 0.5002 - acc: 0.9337 - val_loss: 0.6245 - val_acc: 0.8080\n",
      "Train on 14799 samples, validate on 2019 samples\n",
      "Epoch 1/2\n",
      "14799/14799 [==============================] - 230s - loss: 1.3648 - acc: 0.8928 - val_loss: 0.9067 - val_acc: 0.6677\n",
      "Epoch 2/2\n",
      "14799/14799 [==============================] - 164s - loss: 0.5005 - acc: 0.9333 - val_loss: 1.0464 - val_acc: 0.5597\n",
      "Train on 14438 samples, validate on 2380 samples\n",
      "Epoch 1/2\n",
      "14438/14438 [==============================] - 137s - loss: 1.2844 - acc: 0.8864 - val_loss: 0.4695 - val_acc: 0.8803\n",
      "Epoch 2/2\n",
      "14438/14438 [==============================] - 142s - loss: 0.5072 - acc: 0.9327 - val_loss: 0.4750 - val_acc: 0.8765\n",
      "Train on 14639 samples, validate on 2179 samples\n",
      "Epoch 1/2\n",
      "14639/14639 [==============================] - 149s - loss: 1.3622 - acc: 0.8985 - val_loss: 0.2713 - val_acc: 0.9362\n",
      "Epoch 2/2\n",
      "14639/14639 [==============================] - 149s - loss: 0.5317 - acc: 0.9292 - val_loss: 0.2389 - val_acc: 0.9289\n",
      "Train on 14811 samples, validate on 2007 samples\n",
      "Epoch 1/2\n",
      "14811/14811 [==============================] - 160s - loss: 1.3548 - acc: 0.8854 - val_loss: 0.4240 - val_acc: 0.8819\n",
      "Epoch 2/2\n",
      "14811/14811 [==============================] - 171s - loss: 0.5252 - acc: 0.9247 - val_loss: 0.3705 - val_acc: 0.8949\n",
      "Train on 14543 samples, validate on 2275 samples\n",
      "Epoch 1/2\n",
      "14543/14543 [==============================] - 156s - loss: 1.3917 - acc: 0.8910 - val_loss: 2.0425 - val_acc: 0.4963\n",
      "Epoch 2/2\n",
      "14543/14543 [==============================] - 158s - loss: 0.5180 - acc: 0.9303 - val_loss: 2.4406 - val_acc: 0.4879\n",
      "Train on 16779 samples, validate on 39 samples\n",
      "Epoch 1/2\n",
      "16779/16779 [==============================] - 165s - loss: 1.2057 - acc: 0.8934 - val_loss: 0.1020 - val_acc: 0.9744\n",
      "Epoch 2/2\n",
      "16779/16779 [==============================] - 185s - loss: 0.5339 - acc: 0.9275 - val_loss: 0.0019 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "nr_epochs = 2\n",
    "\n",
    "np.random.seed(123)\n",
    "histories, test_accuracies_list, models = [], [], []\n",
    "for j in range(len(Xs)):\n",
    "    X_train, y_train, X_test, y_test = split_train_test(Xs, ys, j)\n",
    "    model_copy = get_fresh_copy(best_model, best_params['learning_rate'])\n",
    "    datasize = X_train.shape[0]\n",
    "    \n",
    "    history = model_copy.fit(X_train[:datasize,:,:], y_train[:datasize,:],\n",
    "              nb_epoch=nr_epochs, validation_data=(X_test, y_test))\n",
    "    \n",
    "    histories.append(history)\n",
    "    test_accuracies_list.append(history.history['val_acc'][-1] )\n",
    "    models.append(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771778561345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.79675174027750495,\n",
       " 0.59344978186761443,\n",
       " 0.80800542740841252,\n",
       " 0.55968301140653909,\n",
       " 0.87647058833546998,\n",
       " 0.9288664525011473,\n",
       " 0.89486796237012312,\n",
       " 0.48791208793828778,\n",
       " 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(test_accuracies_list))\n",
    "test_accuracies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracies_list = [0.79675174027750495,\n",
    " 0.59344978186761443,\n",
    " 0.80800542740841252,\n",
    " 0.55968301140653909,\n",
    " 0.87647058833546998,\n",
    " 0.9288664525011473,\n",
    " 0.89486796237012312,\n",
    " 0.48791208793828778,\n",
    " 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  0\n",
      "fold  1\n",
      "fold  2\n",
      "fold  3\n",
      "fold  4\n",
      "fold  5\n",
      "fold  6\n",
      "fold  7\n",
      "fold  8\n"
     ]
    }
   ],
   "source": [
    "# Calculate 1-NN for each fold:\n",
    "nr_epochs = 2\n",
    "\n",
    "np.random.seed(123)\n",
    "knn_test_accuracies_list = []\n",
    "for j in range(len(Xs)):\n",
    "    print(\"fold \", j)\n",
    "    X_train, y_train, X_test, y_test = split_train_test(Xs, ys, j)\n",
    "    acc = find_architecture.kNN_accuracy(X_train, y_train, X_test, y_test, k=1)\n",
    "    knn_test_accuracies_list.append(acc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53974709837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN</th>\n",
       "      <th>kNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796752</td>\n",
       "      <td>0.611601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593450</td>\n",
       "      <td>0.610044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.808005</td>\n",
       "      <td>0.613976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.559683</td>\n",
       "      <td>0.523031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.615966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.928866</td>\n",
       "      <td>0.523176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.894868</td>\n",
       "      <td>0.603886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.487912</td>\n",
       "      <td>0.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CNN       kNN\n",
       "0  0.796752  0.611601\n",
       "1  0.593450  0.610044\n",
       "2  0.808005  0.613976\n",
       "3  0.559683  0.523031\n",
       "4  0.876471  0.615966\n",
       "5  0.928866  0.523176\n",
       "6  0.894868  0.603886\n",
       "7  0.487912  0.371429\n",
       "8  1.000000  0.384615"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(knn_test_accuracies_list))\n",
    "accs_compared = pd.DataFrame({'CNN': test_accuracies_list, 'kNN':knn_test_accuracies_list})\n",
    "accs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, loading and comparing reloaded model with orignal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modoel can be saved for future use. The savemodel function will save two separate files: a json file for the architecture and a npy (numpy array) file for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelname = 'my_bestmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    storage.savemodel(model,resultpath,modelname+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mcfly]",
   "language": "python",
   "name": "conda-env-mcfly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
