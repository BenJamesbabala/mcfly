{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional, loguniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Flatten, MaxPooling1D, Lambda, Convolution2D, Flatten, Reshape, LSTM, Dropout, TimeDistributed, Permute\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = '/media/sf_VBox_Shared/timeseries/UCR_WaveGesture/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(datapath, n=100):\n",
    "    X_train = np.load(datapath+'X_train.npy')\n",
    "    y_train_binary = np.load(datapath+'y_train_binary.npy')\n",
    "    X_val = np.load(datapath+'X_val.npy')\n",
    "    y_val_binary = np.load(datapath+'y_val_binary.npy')\n",
    "    X_test = np.load(datapath+'X_test.npy')\n",
    "    y_test_binary = np.load(datapath+'y_test_binary.npy')\n",
    "    return X_train[:n,:,:], y_train_binary[:n,:,:], X_test[:n,:,:], y_val_binary[:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_val):\n",
    "    _, dim_length, dim_channels = X_train.shape # number of samples in a time series\n",
    "    _, outputdim = y_train.shape # number of classes\n",
    "   \n",
    "    min_layers=1 \n",
    "    max_layers=10,\n",
    "    filter_sizes = [16, 32, 64, 128]\n",
    "    min_fc_nodes=10\n",
    "    max_fc_nodes=100\n",
    "    lr_low = -4\n",
    "    lr_high = -1\n",
    "    reg_low =  -4\n",
    "    reg_high = -1\n",
    "    nr_epochs = 5\n",
    "\n",
    "    num_layers = {{choice(list(range(min_layers, max_layers+1)))}}\n",
    "    learning_rate = {{loguniform(lr_low, lr_high)}}\n",
    "    regularization_rate = {{loguniform(reg_low, reg_high)}}\n",
    "    \n",
    "    model = Sequential()\n",
    "    filter_0 = {{choice(filter_sizes)}}\n",
    "    model.add(\n",
    "        Convolution1D(filters[0], 3, border_mode='same', input_shape=(dim_length, dim_channels),\n",
    "                      W_regularizer=l2(regularization_rate)))\n",
    "    for l in range(1, num_layers):\n",
    "        filter_l = {{choice(filter_sizes)}}\n",
    "        model.add(Convolution1D(filter_number, 3, border_mode='same', W_regularizer=l2(regularization_rate)))\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    fc_hidden_nodes = {{choice(list(range(min_fc_nodes, max_fc_nodes+1)))}}\n",
    "    model.add(Dense(output_dim=fc_hidden_nodes, W_regularizer=l2(regularization_rate)))  # Fully connected layer\n",
    "    model.add(Activation('relu'))  # Relu activation\n",
    "    model.add(Dense(output_dim=outputdim))\n",
    "    model.add(Activation(\"softmax\"))  # Final classification layer\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train,\n",
    "                            nb_epoch=nr_epochs, batch_size=20, # see comment on subsize_set\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            show_accuracy=True,\n",
    "                            verbose=2)\n",
    "    score, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTS:\n",
      "num_layers\n",
      "learning_rate\n",
      "regularization_rate\n",
      "filter_0\n",
      "filter_0\n",
      "fc_hidden_nodes\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'num_layers': hp.choice('num_layers', list(range(min_layers, max_layers+1))),\n",
      "        'learning_rate': hp.loguniform('learning_rate', lr_low, lr_high),\n",
      "        'regularization_rate': hp.loguniform('regularization_rate', reg_low, reg_high),\n",
      "        'filter_0': hp.choice('filter_0', filter_sizes),\n",
      "        'filter_0_1': hp.choice('filter_0_1', filter_sizes),\n",
      "        'fc_hidden_nodes': hp.choice('fc_hidden_nodes', list(range(min_fc_nodes, max_fc_nodes+1))),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "\n",
      "\n",
      "\n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "def keras_fmin_fnct(space):\n",
      "\n",
      "    _, dim_length, dim_channels = X_train.shape # number of samples in a time series\n",
      "    _, outputdim = y_train.shape # number of classes\n",
      "   \n",
      "    min_layers=1 \n",
      "    max_layers=10,\n",
      "    filter_sizes = [16, 32, 64, 128]\n",
      "    min_fc_nodes=10\n",
      "    max_fc_nodes=100\n",
      "    lr_low = -4\n",
      "    lr_high = -1\n",
      "    reg_low =  -4\n",
      "    reg_high = -1\n",
      "\n",
      "    num_layers = space['num_layers']\n",
      "    learning_rate = space['learning_rate']\n",
      "    regularization_rate = space['regularization_rate']\n",
      "    \n",
      "    model = Sequential()\n",
      "    filter_0 = space['filter_0']\n",
      "    model.add(\n",
      "        Convolution1D(filters[0], 3, border_mode='same', input_shape=(dim_length, dim_channels),\n",
      "                      W_regularizer=l2(regularization_rate)))\n",
      "    for l in range(1, num_layers):\n",
      "        filter_l = space['filter_0_1']\n",
      "        model.add(Convolution1D(filter_number, 3, border_mode='same', W_regularizer=l2(regularization_rate)))\n",
      "        model.add(Activation('relu'))\n",
      "    model.add(Flatten())\n",
      "    fc_hidden_nodes = space['fc_hidden_nodes']\n",
      "    model.add(Dense(output_dim=fc_hidden_nodes, W_regularizer=l2(regularization_rate)))  # Fully connected layer\n",
      "    model.add(Activation('relu'))  # Relu activation\n",
      "    model.add(Dense(output_dim=outputdim))\n",
      "    model.add(Activation(\"softmax\"))  # Final classification layer\n",
      "\n",
      "    model.compile(loss='categorical_crossentropy',\n",
      "                  optimizer=Adam(lr=learning_rate),\n",
      "                  metrics=['accuracy'])\n",
      "    history = model.fit(X_train, y_train,\n",
      "                            nb_epoch=nr_epochs, batch_size=20, # see comment on subsize_set\n",
      "                            validation_data=(X_val, y_val),\n",
      "                            show_accuracy=True,\n",
      "                            verbose=2)\n",
      "    score, acc = model.evaluate(X_val, y_val, verbose=0)\n",
      "    print('Test accuracy:', acc)\n",
      "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "\n",
      "\n",
      "Unexpected error: <class 'SyntaxError'>\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (temp_model.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"/home/dafne/timeseries/mcfly/notebooks/temp_model.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    importers when locating support scripts as well as when importing modules.\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=lambda: data(datasets),\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
